{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"collapsed_sections":["f24b75b3","cc94a774","7b3f1d57","6d83352b","05818b09"]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7749513,"sourceType":"datasetVersion","datasetId":4530578}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"6f3ec6892726467ebe3fae7f102fae16":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_22018e6ad60c48dfa066b054320081e6","IPY_MODEL_b2640b2b37be445e937ca6eea99087e7","IPY_MODEL_bf7d55d6228949a9a2363050553a79ce"],"layout":"IPY_MODEL_63bfa208041a41bb9ad6068bed803a45"}},"22018e6ad60c48dfa066b054320081e6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4dafa2ee5faf4509912f7a68744d5a90","placeholder":"​","style":"IPY_MODEL_57a6ed1d171840c1af8383198f0ec4f1","value":"Map: 100%"}},"b2640b2b37be445e937ca6eea99087e7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3b411705e7947d7a534cc67e75a7b5e","max":999,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b69da5d4e1384585a28d6fefef280137","value":999}},"bf7d55d6228949a9a2363050553a79ce":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aaf7271fef0b4032901703c97002c610","placeholder":"​","style":"IPY_MODEL_8aea77791d3340c6ba4e236739b5a0c3","value":" 999/999 [00:00&lt;00:00, 1656.49 examples/s]"}},"63bfa208041a41bb9ad6068bed803a45":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4dafa2ee5faf4509912f7a68744d5a90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57a6ed1d171840c1af8383198f0ec4f1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d3b411705e7947d7a534cc67e75a7b5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b69da5d4e1384585a28d6fefef280137":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"aaf7271fef0b4032901703c97002c610":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8aea77791d3340c6ba4e236739b5a0c3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Install Dependencies","metadata":{"id":"f24b75b3","jp-MarkdownHeadingCollapsed":true,"tags":[]}},{"cell_type":"code","source":"!pip install -q -U datasets\n!pip install -q -U accelerate\n!pip install -q -U sentencepiece\n!pip install -q -U git+https://github.com/huggingface/transformers.git\n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install git+https://github.com/boudinfl/pke.git\n!pip install nltk fuzzywuzzy python-Levenshtein\n!python -m spacy download en_core_web_sm\n!pip install nltk==3.5.0\n!pip install sense2vec==2.0.1\n!wget https://github.com/explosion/sense2vec/releases/download/v1.0.0/s2v_reddit_2015_md.tar.gz\n!tar -xvf  s2v_reddit_2015_md.tar.gz\n!pip install keybert\n!pip install keybert[flair]\n!pip install keybert[gensim]\n!pip install keybert[spacy]\n!pip install keybert[use]\n!pip install python-pptx\n!pip install python-docx\n!pip install openpyxl\n!pip install rarfile\n!pip install PyPDF2\n!pip install xlrd","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ac09373e","outputId":"2ceefb62-3932-4022-a845-217517132e47","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nif torch.cuda.is_available()==True :\n    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"","metadata":{"id":"c06f6c13","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### dealing with the data","metadata":{"id":"cc94a774","jp-MarkdownHeadingCollapsed":true,"tags":[]}},{"cell_type":"code","source":"import PyPDF2\nimport requests\nfrom bs4 import BeautifulSoup\nimport csv\nimport zipfile\nimport json\nimport xml.etree.ElementTree as ET\nimport docx\nimport pptx\nimport rarfile\nimport xlrd  # For XLS files\nimport openpyxl  # For XLSX files\nimport shutil  # For extracting ZIP files\n\ndef read_csv(file_path):\n    with open(file_path, 'r', newline='') as csvfile:\n        csv_reader = csv.reader(csvfile)\n        csv_data = [row for row in csv_reader]\n    return csv_data\n\ndef read_text(file_path):\n    with open(file_path, 'r') as f:\n        text_data = f.read()\n    return text_data\n\ndef read_pdf(file_path):\n    pdf_file = open(file_path, 'rb')\n    pdf_reader = PyPDF2.PdfReader(pdf_file)\n    text_data = []\n    for page in pdf_reader.pages:\n        text_data.append(page.extract_text())\n    return text_data\n\ndef read_web_page(url):\n    result = requests.get(url)\n    src = result.content\n    soup = BeautifulSoup(src, 'html.parser')\n    text_data = ''\n    for p in soup.find_all('p'):\n        text_data += p.get_text() + '\\n'\n    return text_data\n\ndef read_docx(file_path):\n    doc = docx.Document(file_path)\n    text_data = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n    return text_data\n\ndef read_pptx(file_path):\n    ppt = pptx.Presentation(file_path)\n    text_data = ''\n    for slide in ppt.slides:\n        for shape in slide.shapes:\n            if hasattr(shape, \"text\"):\n                text_data += shape.text + '\\n'\n    return text_data\n\ndef read_xlsx(file_path):\n    workbook = openpyxl.load_workbook(file_path)\n    sheet = workbook.active\n    text_data = ''\n    for row in sheet.iter_rows(values_only=True):\n        text_data += ' '.join([str(cell) for cell in row if cell is not None]) + '\\n'\n    return text_data\n\n\ndef read_json(file_path):\n    with open(file_path, 'r') as f:\n        json_data = json.load(f)\n    return json_data\n\ndef read_html(file_path):\n    with open(file_path, 'r') as f:\n        html_data = f.read()\n    return html_data\n\ndef read_xml(file_path):\n    tree = ET.parse(file_path)\n    root = tree.getroot()\n    return ET.tostring(root, encoding='unicode')\n\ndef read_zip(file_path):\n    file_contents = []\n    with zipfile.ZipFile(file_path, 'r') as zip_ref:\n        for file_info in zip_ref.infolist():\n            with zip_ref.open(file_info) as file:\n                # Call read_data to handle reading and processing the file contents\n                file_data = read_data(file)\n                file_contents.append(file_data)\n    return file_contents\n\ndef read_rar(file_path):\n    file_contents = []\n    with rarfile.RarFile(file_path, 'r') as rar_ref:\n        for rar_info in rar_ref.infolist():\n            with rar_ref.open(rar_info) as file:\n                # Call read_data to handle reading and processing the file contents\n                file_data = read_data(file)\n                file_contents.append(file_data)\n    return file_contents\n\n\ndef read_data(file_path):\n    # Check if the file is a CSV file\n    if file_path.endswith('.csv'):\n        return read_csv(file_path)\n\n    # Check if the file is a text file\n    elif file_path.endswith('.txt'):\n        return read_text(file_path)\n\n    # Check if the file is a PDF file\n    elif file_path.endswith('.pdf'):\n        return read_pdf(file_path)\n\n    # Check if the file is a DOCX file\n    elif file_path.endswith('.docx'):\n        return read_docx(file_path)\n\n    # Check if the file is a PPTX file\n    elif file_path.endswith('.pptx'):\n        return read_pptx(file_path)\n\n    # Check if the file is an XLSX file\n    elif file_path.endswith('.xlsx'):\n        return read_xlsx(file_path)\n\n    # Check if the file is a JSON file\n    elif file_path.endswith('.json'):\n        return read_json(file_path)\n\n    # Check if the file is an HTML file\n    elif file_path.endswith('.html'):\n        return read_html(file_path)\n\n    # Check if the file is an XML file\n    elif file_path.endswith('.xml'):\n        return read_xml(file_path)\n\n    # Check if the file is a ZIP file\n    elif file_path.endswith('.zip'):\n        return read_zip(file_path)\n\n    # Check if the file is a RAR file\n    elif file_path.endswith('.rar'):\n        return read_rar(file_path)\n\n    # # Assume it's a web page if it's not a file\n    # elif os.path.exists(file_path):\n    #     return read_text(file_path)\n\n    # Treat it as a web page if it's a URL\n    elif file_path.startswith('http'):\n        return read_web_page(file_path)\n\n    # If the file type is unknown, return None\n    else:\n        print(\"Unsupported file type\")\n        return None\n\n# Example usage:\nfile_path = 'https://huggingface.co/docs/transformers/quicktour'\nexample = read_data(file_path)\nexample","metadata":{"id":"e5d2c018","outputId":"2b9e6a2e-f656-4d26-c394-711a33afc344","tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cleaning the data","metadata":{"id":"7b3f1d57","jp-MarkdownHeadingCollapsed":true,"tags":[]}},{"cell_type":"code","source":"import re\nimport string\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('omw-1.4')\nnltk.download('wordnet')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet as wn\nstop_words = stopwords.words('english')\n# arabic_stopwords = stopwords.words('arabic')","metadata":{"id":"a09e9eea","outputId":"7e8f2529-64d8-4163-8421-2afd1d226078","tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_non_ascii(text):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    return text.encode('ascii','ignore').decode()\n    #return ''.join(char for char in text if char.isalpha() and char.isnumeric() or 'ARABIC' in unicodedata.name(char, ''))\n\ndef remove_brackets_num(text):\n    return re.sub(\"\\*?\",\"\",text)\n\ndef to_lowercase(text):\n    return text.lower()\n\ndef replace_numbers(text):\n    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n    return re.sub(r'\\d+','',text)\n\ndef remove_whitespace(text):\n      return text.strip()\n\ndef remove_punctuation(text):\n    punctuation= '''!()[]{};:'\"\\<>/?$%^&*_`~='''\n    for punc in punctuation:\n        text=text.replace(punc,\"\")\n    return text\n\ndef remove_emails(text):\n    return re.sub(r'[A-Za-z0-9]*@[A-Za-z]*\\.?[A-Za-z0-9]*', \"\", text)\n\ndef text2words(text):\n    return word_tokenize(text)\n\ndef remove_stopwords(words,stop_words):\n    return [word for word in words if word not in stop_words]\n\n\ndef normalize_text(text):\n    text = remove_non_ascii(text)\n    text= remove_brackets_num(text)\n    text = to_lowercase(text)\n    #text=replace_numbers(text)\n    text= remove_whitespace(text)\n    text = remove_punctuation(text)\n    text= remove_emails(text)\n    words = text2words(text)\n    #words = remove_stopwords(words, stop_words)\n\n    return ' '.join(words)\n\nN_text_file=normalize_text(example)\n\ndef divide_text_into_chunks(text, chunk_size):\n    chunks = []\n    if len(text) <= chunk_size:\n        return [text]  # Return the whole text as a single chunk\n    for i in range(0, len(text), chunk_size):\n        chunk = text[i:i+chunk_size]\n        chunks.append(chunk)\n    return chunks\n\nchunks=divide_text_into_chunks(N_text_file,1024)","metadata":{"id":"cb1c452e","outputId":"416f1537-7f54-4f58-c069-98b3c51dc4b1","tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generate KeyWords","metadata":{"id":"6d83352b","jp-MarkdownHeadingCollapsed":true,"tags":[]}},{"cell_type":"code","source":"from keybert import KeyBERT\nkw_model = KeyBERT()","metadata":{"colab":{"referenced_widgets":["df9e50b5975140d89e3887fd3b19043b","c05eaf0feb09443f94d6d14e830a3674","777fbe69d66549fd86336383227a514c","d02adbccf865442c9def41fc161de68e","82964ee98852448fba330e0f2edccafb","47a5dba3bcb248a390cfb89b2b1f777e","3812140ada794c62bdab1f02c09d1366","9bc760c1d55243689d9b105af8f660bf","94e5edd81666435da1eefadf353e4c09","012b28ce058b4501a933eed641ad06c0","17577032c6bf4b6cb72e8c840f34e8fb"]},"id":"4c066f60","outputId":"ce3fde9e-945b-4ee4-c13d-93fa89482689","tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def jaccard_similarity(word1, word2):\n    set1 = set(word1)\n    set2 = set(word2)\n\n    intersection = len(set1.intersection(set2))\n    union = len(set1.union(set2))\n\n    similarity = intersection / union if union > 0 else 0\n    return similarity\ndef extract_keywords_from_chunks(chunks):\n    chunk_keywords_list = []\n    all_keywords = set()  # Set to store all keywords seen so far\n    similarity_threshold_within_chunk = 0.4  # Threshold for similarity within a chunk\n    similarity_threshold_between_chunks = 0.4  # Threshold for similarity between chunks\n\n    for chunk in chunks:\n        # Extract keywords from the current chunk\n        KeyBERT1 = kw_model.extract_keywords(chunk, keyphrase_ngram_range=(1,1), top_n=1)\n        KeyBERT2 = kw_model.extract_keywords(chunk, keyphrase_ngram_range=(2,2), top_n=1)\n        KeyBERT3 = kw_model.extract_keywords(chunk, keyphrase_ngram_range=(3,3), top_n=1)\n\n        # Combine all extracted keywords\n        chunk_keywords = [key[0] for key in KeyBERT1] + \\\n                         [key[0] for key in KeyBERT2] + \\\n                         [key[0] for key in KeyBERT3]\n        # Filter out empty chunks and corresponding keywords\n        if chunk_keywords:\n            # Filter out very similar keywords within the current chunk\n            unique_keywords_within_chunk = []\n            for keyword in chunk_keywords:\n                if all(jaccard_similarity(keyword, existing_keyword) < similarity_threshold_within_chunk for existing_keyword in unique_keywords_within_chunk):\n                    unique_keywords_within_chunk.append(keyword)\n\n            # Filter out very similar keywords between the current chunk and previously processed chunks\n            unique_keywords_between_chunks = []\n            for keyword in unique_keywords_within_chunk:\n                if all(jaccard_similarity(keyword, existing_keyword) < similarity_threshold_between_chunks for _, existing_keywords in chunk_keywords_list for existing_keyword in existing_keywords):\n                    unique_keywords_between_chunks.append(keyword)\n\n            # Check if there are any keywords left after filtering\n            if unique_keywords_between_chunks:\n                chunk_keywords_list.append((chunk, unique_keywords_between_chunks))\n                all_keywords.update(unique_keywords_between_chunks)\n\n    return chunk_keywords_list\n\nchunks_with_keywords = extract_keywords_from_chunks(chunks)","metadata":{"id":"c7a1388c","outputId":"ddd01415-06d2-41eb-e2f3-8fbafb9ff50d","tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generate Distractors","metadata":{"id":"05818b09","jp-MarkdownHeadingCollapsed":true,"tags":[]}},{"cell_type":"code","source":"from sense2vec import Sense2Vec\n# load sense2vec vectors\ns2v = Sense2Vec().from_disk('s2v_old')","metadata":{"id":"86fe680f","tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import OrderedDict\n\ndef sense2vec_get_words(word, s2v):\n    output = []\n    word = word.lower()\n    word = word.replace(\" \", \"_\")\n\n    sense = s2v.get_best_sense(word)\n    out = []\n    if sense is not None:\n        most_similar = s2v.most_similar(sense, n=20)\n\n        for each_word in most_similar:\n            append_word = each_word[0].split(\"|\")[0].replace(\"_\", \" \").lower()\n\n            # Check similarity and exclude if too similar\n            similarity_threshold = 0.4\n            if append_word != word and jaccard_similarity(word, append_word) < similarity_threshold:\n                output.append(append_word.title())\n\n        out = list(OrderedDict.fromkeys(output))\n    return out[:3]\n\nchunk_keyword_distractor_list = []\nfor chunk, keywords in chunks_with_keywords:\n    # Generate distractors for each keyword\n    result = {word: sense2vec_get_words(word, s2v) for word in keywords}\n\n    # Filter out keywords without distractors or with fewer than three distractors\n    filtered_keywords = []\n    for word, distractors in result.items():\n        if distractors and len(distractors) >= 3:  # Check if distractors list contains at least three words\n            filtered_keywords.append(word)\n\n    # If there are keywords with filtered distractors, add them to the list\n    if filtered_keywords:\n        chunk_keyword_distractor_list.append((chunk, filtered_keywords, {word: result[word] for word in filtered_keywords}))\n\n# Output the list of tuples\nfor chunk_data in chunk_keyword_distractor_list:\n    print(\"Chunk:\", chunk_data[0])\n    print(\"Keywords:\", chunk_data[1])\n    print(\"Distractors:\")\n    for keyword, distractors in chunk_data[2].items():\n        print(f\"  Distractors for {keyword}: {distractors}\")\n    print()","metadata":{"id":"f7db9293","outputId":"a5108c31-4552-4223-8ef1-229ec78b7bf2","tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### loading the model and the tokenizer","metadata":{"id":"f30fd042","tags":[]}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel_name_or_path = \"facebook/bart-base\"\nG_model = AutoModelForSeq2SeqLM.from_pretrained(\n    model_name_or_path,\n    torch_dtype=torch.float32,\n    device_map='auto',\n)\nG_tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4c008ac5-d6ca-4569-a4d7-d6bf53503205","outputId":"c91ef1cf-e4c7-4086-c297-3f02a4316622","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Freeze all layers in the encoder\nfor layer in G_model.model.encoder.layers:\n    for parameter in layer.parameters():\n        parameter.requires_grad = False\n\n# Freeze all layers in the decoder\nfor layer in G_model.model.decoder.layers:\n    for parameter in layer.parameters():\n        parameter.requires_grad = False","metadata":{"id":"702c2901","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, TaskType, get_peft_model\nLora_config = LoraConfig(\n    r=18,\n    lora_alpha=8,\n    target_modules=[\"v_proj\" , \"q_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.SEQ_2_SEQ_LM\n)\n\nL_model = get_peft_model(G_model, Lora_config)\nprint(L_model.print_trainable_parameters())","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dba4b9f7","outputId":"8c3ff1fb-3fc8-470d-b489-130ae05da02e","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset, load_dataset, concatenate_datasets\n\n#for General english data\nen_g_train = (\n    load_dataset(\"squad_v2\", split=\"train\")\n    .filter(lambda example: len(example[\"answers\"][\"text\"]) > 0 and example[\"context\"] and example[\"question\"])\n    .shuffle(seed=42)\n    .select(range(7300))\n    .map(lambda example: {\"context\": example[\"context\"], \"question\": example[\"question\"], \"answer\": example[\"answers\"][\"text\"][0]})\n    .remove_columns([\"id\", \"title\", \"answers\"])\n)\nen_g_validation = (\n    load_dataset(\"squad_v2\", split=\"validation\")\n    .filter(lambda example: len(example[\"answers\"][\"text\"]) > 0 and example[\"context\"] and example[\"question\"])\n    .shuffle(seed=42)\n    .select(range(382))\n    .map(lambda example: {\"context\": example[\"context\"], \"question\": example[\"question\"], \"answer\": example[\"answers\"][\"text\"][0]})\n    .remove_columns([\"id\", \"title\", \"answers\"])\n)\n\n#for diff english data\nen_d_train = (\n    load_dataset(\"drop\", split=\"train\")\n    .filter(lambda example: len(example[\"answers_spans\"][\"spans\"]) > 0 and example[\"passage\"] and example[\"question\"])\n    .shuffle(seed=42)\n    .select(range(7300))\n    .map(lambda example: {\"context\": example[\"passage\"], \"question\": example[\"question\"], \"answer\": example[\"answers_spans\"][\"spans\"][0]})\n    .remove_columns([\"section_id\", \"query_id\", \"answers_spans\", \"passage\"])\n)\nen_d_validation = (\n    load_dataset(\"drop\", split=\"validation\")\n    .filter(lambda example: len(example[\"answers_spans\"][\"spans\"]) > 0 and example[\"passage\"] and example[\"question\"])\n    .shuffle(seed=42)\n    .select(range(383))\n    .map(lambda example: {\"context\": example[\"passage\"], \"question\": example[\"question\"], \"answer\": example[\"answers_spans\"][\"spans\"][0]})\n    .remove_columns([\"section_id\", \"query_id\", \"answers_spans\", \"passage\"])\n)\n\n# #for IT english data\nen_it_train = (\n    load_dataset(\"mou3az/IT_QA-QG\", split=\"train\")\n    .filter(lambda example: len(example[\"answer\"]) > 0 and example[\"context\"] and example[\"question\"])\n    .shuffle(seed=42)\n    # .select(range(20000))\n    .remove_columns([\"id\"])\n)\nen_it_validation = (\n    load_dataset(\"mou3az/IT_QA-QG\", split=\"validation\")\n    .filter(lambda example: len(example[\"answer\"]) > 0 and example[\"context\"] and example[\"question\"])\n    .shuffle(seed=42)\n    # .select(range(1000))\n    .remove_columns([\"id\"])\n)\n\n# # Mix the datasets\nmixed_train = concatenate_datasets([en_g_train, en_it_train, en_d_train])\n\n# Shuffle the mixed dataset\nen_train = mixed_train.shuffle(seed=123)\n\n# Mix the datasets\nmixed_validation = concatenate_datasets([en_g_validation, en_it_validation, en_d_validation])\n\n# Shuffle the mixed dataset\nen_validation = mixed_validation.shuffle(seed=123)","metadata":{"id":"55751b0d","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#For General data\ndef create_prompt1(context, answer):\n    input_text = f\"Given the context '{context}' and the answer '{answer}', what question can be asked?\"\n    return input_text\n\ndef create_prompt2(question):\n    output_text = f\"question: {question}\"\n    return output_text","metadata":{"id":"85d18ac3","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for english data\nen_train_data = en_train.map(lambda samples: G_tokenizer.encode_plus(create_prompt1(samples['context'], samples['answer']), padding=True, truncation=True), remove_columns=[\"context\", \"answer\", \"question\"])\nen_validation_data = en_validation.map(lambda samples: G_tokenizer.encode_plus(create_prompt1(samples['context'], samples['answer']), padding=True, truncation=True), remove_columns=[\"context\", \"answer\", \"question\"])\nen_question_Tdata = en_train.map(lambda samples: G_tokenizer.encode_plus(create_prompt2(samples['question']), padding=True, truncation=True), remove_columns=[\"context\", \"answer\", \"question\"])[\"input_ids\"]\nen_question_Vdata = en_validation.map(lambda samples: G_tokenizer.encode_plus(create_prompt2(samples['question']), padding=True, truncation=True), remove_columns=[\"context\", \"answer\", \"question\"])[\"input_ids\"]\nen_train_data=en_train_data.add_column(\"labels\", en_question_Tdata)\nen_validation_data=en_validation_data.add_column(\"labels\", en_question_Vdata)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["6f3ec6892726467ebe3fae7f102fae16","22018e6ad60c48dfa066b054320081e6","b2640b2b37be445e937ca6eea99087e7","bf7d55d6228949a9a2363050553a79ce","63bfa208041a41bb9ad6068bed803a45","4dafa2ee5faf4509912f7a68744d5a90","57a6ed1d171840c1af8383198f0ec4f1","d3b411705e7947d7a534cc67e75a7b5e","b69da5d4e1384585a28d6fefef280137","aaf7271fef0b4032901703c97002c610","8aea77791d3340c6ba4e236739b5a0c3"]},"id":"c60d0004","outputId":"9798760e-5a66-4c4f-f49b-a0041f02dda9","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, EarlyStoppingCallback, Seq2SeqTrainer\n\ntraining_args = Seq2SeqTrainingArguments(\n      gradient_accumulation_steps=24,\n      per_device_train_batch_size=8,\n      per_device_eval_batch_size=8,\n      # save_steps=2,\n      eval_steps=1,\n      warmup_steps=50,\n      logging_steps=1,\n      weight_decay=0.05,\n      # save_total_limit=5,\n      learning_rate=3e-3,\n      max_steps=1000,\n      # num_train_epochs=2,\n      # load_best_model_at_end=True,\n      # gradient_checkpointing=True,\n      lr_scheduler_type=\"linear\",\n      do_train=True,\n      do_eval=True,\n      # fp16=False,\n      report_to=\"all\",\n      log_level=\"debug\",\n      logging_dir='./logs',\n      output_dir='./outputs',\n      label_names=[\"labels\"],\n      evaluation_strategy=\"steps\",\n      metric_for_best_model=\"eval_loss\",\n    )\n\ntrainer = Seq2SeqTrainer(\n    model=L_model,\n    args=training_args,\n    tokenizer=G_tokenizer,\n    train_dataset=en_train_data,\n    eval_dataset=en_validation_data,\n    # callbacks=[EarlyStoppingCallback(2, 1.0)],\n    data_collator=DataCollatorForSeq2Seq(G_tokenizer,label_pad_token_id=-100),\n)\n\n# Additional configuration\nL_model.config.use_cache = False\ntorch.cuda.empty_cache()\n\n# Start training\ntrainer.train()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"16c9fd76","outputId":"5d6528e0-decb-48fe-c371-cfcdc6bbf84c","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to hugging face\nmodel_name = \"\"\nHUGGING_FACE_USER_NAME = \"\"\n\nG_model.push_to_hub(f\"{HUGGING_FACE_USER_NAME}/{model_name}\", token='')","metadata":{"id":"1d90c56e","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save model checkpoint\nL_model.save_pretrained(\"\")\n# Create a zip archive\n!zip -r saved_model.zip \n# Create a rar archive\n!rar a saved_model.rar ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#general question generation model\nimport torch\nfrom peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nHUGGING_FACE_USER_NAME='mou3az'\nmodel_name='IT-General_Question-Generation'\npeft_model_id = f\"{HUGGING_FACE_USER_NAME}/{model_name}\"\nconfig = PeftConfig.from_pretrained(peft_model_id)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=False, device_map='auto')\nG_tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n\n# Load the Lora model\nG_model = PeftModel.from_pretrained(model, peft_model_id)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\nfrom rouge import Rouge\nfrom fuzzywuzzy import fuzz\n\ndef calculate_bleu_scores(references, predictions):\n    return corpus_bleu([[ref.split()] for ref in references], [pred.split() for pred in predictions])\n\ndef calculate_rouge_scores(references, predictions):\n    rouge = Rouge()\n    rouge_scores = rouge.get_scores(predictions, references, avg=True)\n    return rouge_scores['rouge-l']['f']\n\ndef calculate_accuracy(references, predictions):\n    accuracies = [fuzz.token_sort_ratio(ref, pred) / 100.0 for ref, pred in zip(references, predictions)]\n    return sum(accuracies) / len(accuracies)\n\ndef evaluate(dataset):\n    references = [sample['question'] for sample in dataset]\n    predictions = [get_question(sample['context'], sample['answer']) for sample in dataset]  # Assuming 'get_question' generates model's output\n\n    bleu_score = calculate_bleu_scores(references, predictions)\n    rouge_score = calculate_rouge_scores(references, predictions)\n    accuracy = calculate_accuracy(references, predictions)\n\n    print(\"Overall Accuracy:\", accuracy)\n    print(\"Overall BLEU Score:\", bleu_score)\n    print(\"Overall ROUGE Score:\", rouge_score)\n\n# Assuming 'en_validation' is your dataset\nevaluate(en_validation)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_questions_with_distractors(chunk_keyword_distractor_list):\n    device = next(G_model.parameters()).device\n    all_questions = []\n    for chunk, keywords, distractors_dict in chunk_keyword_distractor_list:\n        for keyword in keywords:\n            distractors = distractors_dict.get(keyword, [])\n            input_text = f\"Given the context '{context}' and the answer '{answer}', what question can be asked?\"\n            encoding = G_tokenizer.encode_plus(input_text, padding=True, return_tensors=\"pt\").to(device)\n\n            output_tokens = G_model.generate(**encoding, early_stopping=True, num_beams=5, num_return_sequences=1, no_repeat_ngram_size=2, max_length=100)\n            question = G_tokenizer.decode(output_tokens[0], skip_special_tokens=True).replace(\"question:\", \"\").strip()\n            all_questions.append((chunk, keyword, question, distractors))\n    return all_questions\nTotal_List= generate_questions_with_distractors(chunk_keyword_distractor_list)\n\n# Print the questions\nfor chunk,keyword, question, distractors in Total_List:\n    print(f\"chunk: {chunk}\")\n    print(f\"Keyword: {keyword}\")\n    print(f\"Question: {question}\")\n    print(f\"Distractors: {distractors}\")\n    print()","metadata":{},"execution_count":null,"outputs":[]}]}
