{
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "pYfKMpAjRLDr",
        "cc94a774",
        "7b3f1d57",
        "6d83352b",
        "05818b09",
        "exLV1HPzRLEG",
        "h0qGw8h7RSqE"
      ]
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 7749513,
          "sourceType": "datasetVersion",
          "datasetId": 4530578
        },
        {
          "sourceId": 7770645,
          "sourceType": "datasetVersion",
          "datasetId": 4545907
        },
        {
          "sourceId": 7771818,
          "sourceType": "datasetVersion",
          "datasetId": 4546679
        },
        {
          "sourceId": 7792920,
          "sourceType": "datasetVersion",
          "datasetId": 4561912
        },
        {
          "sourceId": 7795842,
          "sourceType": "datasetVersion",
          "datasetId": 4564047
        },
        {
          "sourceId": 7796098,
          "sourceType": "datasetVersion",
          "datasetId": 4564227
        }
      ],
      "dockerImageVersionId": 30665,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# for training\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install fuzzywuzzy python-Levenshtein\n",
        "!pip install bitsandbytes==0.41.3\n",
        "!pip install -q -U sentencepiece\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U datasets\n",
        "!pip install nltk==3.5.0\n",
        "!pip install bert-score\n",
        "!pip install rouge\n",
        "#for inference time\n",
        "!pip install xlrd\n",
        "!pip install PyPDF2\n",
        "!pip install keybert\n",
        "!pip install rarfile\n",
        "!pip install openpyxl\n",
        "!pip install python-pptx\n",
        "!pip install python-docx\n",
        "!pip install keybert[use]\n",
        "!pip install keybert[spacy]\n",
        "!pip install keybert[flair]\n",
        "!pip install keybert[gensim]\n",
        "!pip install sense2vec==2.0.1\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install git+https://github.com/boudinfl/pke.git\n",
        "!wget https://github.com/explosion/sense2vec/releases/download/v1.0.0/s2v_reddit_2015_md.tar.gz\n",
        "!tar -xvf  s2v_reddit_2015_md.tar.gz"
      ],
      "metadata": {
        "id": "ac09373e",
        "tags": [],
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "ac09373e"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "if torch.cuda.is_available()==True :\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
      ],
      "metadata": {
        "id": "c06f6c13",
        "tags": [],
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "c06f6c13"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# #########################################"
      ],
      "metadata": {
        "id": "SXqhksrXRLDW"
      },
      "id": "SXqhksrXRLDW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For Inference Time"
      ],
      "metadata": {
        "id": "pYfKMpAjRLDr"
      },
      "id": "pYfKMpAjRLDr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### dealing with the data"
      ],
      "metadata": {
        "id": "cc94a774",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "id": "cc94a774"
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import zipfile\n",
        "import json\n",
        "import xml.etree.ElementTree as ET\n",
        "import docx\n",
        "import pptx\n",
        "import rarfile\n",
        "import xlrd  # For XLS files\n",
        "import openpyxl  # For XLSX files\n",
        "import shutil  # For extracting ZIP files\n",
        "\n",
        "def read_csv(file_path):\n",
        "    with open(file_path, 'r', newline='') as csvfile:\n",
        "        csv_reader = csv.reader(csvfile)\n",
        "        csv_data = [row for row in csv_reader]\n",
        "    return csv_data\n",
        "\n",
        "def read_text(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        text_data = f.read()\n",
        "    return text_data\n",
        "\n",
        "def read_pdf(file_path):\n",
        "    pdf_file = open(file_path, 'rb')\n",
        "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "    text_data = []\n",
        "    for page in pdf_reader.pages:\n",
        "        text_data.append(page.extract_text())\n",
        "    return text_data\n",
        "\n",
        "def read_web_page(url):\n",
        "    result = requests.get(url)\n",
        "    src = result.content\n",
        "    soup = BeautifulSoup(src, 'html.parser')\n",
        "    text_data = ''\n",
        "    for p in soup.find_all('p'):\n",
        "        text_data += p.get_text() + '\\n'\n",
        "    return text_data\n",
        "\n",
        "def read_docx(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    text_data = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
        "    return text_data\n",
        "\n",
        "def read_pptx(file_path):\n",
        "    ppt = pptx.Presentation(file_path)\n",
        "    text_data = ''\n",
        "    for slide in ppt.slides:\n",
        "        for shape in slide.shapes:\n",
        "            if hasattr(shape, \"text\"):\n",
        "                text_data += shape.text + '\\n'\n",
        "    return text_data\n",
        "\n",
        "def read_xlsx(file_path):\n",
        "    workbook = openpyxl.load_workbook(file_path)\n",
        "    sheet = workbook.active\n",
        "    text_data = ''\n",
        "    for row in sheet.iter_rows(values_only=True):\n",
        "        text_data += ' '.join([str(cell) for cell in row if cell is not None]) + '\\n'\n",
        "    return text_data\n",
        "\n",
        "\n",
        "def read_json(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        json_data = json.load(f)\n",
        "    return json_data\n",
        "\n",
        "def read_html(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        html_data = f.read()\n",
        "    return html_data\n",
        "\n",
        "def read_xml(file_path):\n",
        "    tree = ET.parse(file_path)\n",
        "    root = tree.getroot()\n",
        "    return ET.tostring(root, encoding='unicode')\n",
        "\n",
        "def read_zip(file_path):\n",
        "    file_contents = []\n",
        "    with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "        for file_info in zip_ref.infolist():\n",
        "            with zip_ref.open(file_info) as file:\n",
        "                # Call read_data to handle reading and processing the file contents\n",
        "                file_data = read_data(file)\n",
        "                file_contents.append(file_data)\n",
        "    return file_contents\n",
        "\n",
        "def read_rar(file_path):\n",
        "    file_contents = []\n",
        "    with rarfile.RarFile(file_path, 'r') as rar_ref:\n",
        "        for rar_info in rar_ref.infolist():\n",
        "            with rar_ref.open(rar_info) as file:\n",
        "                # Call read_data to handle reading and processing the file contents\n",
        "                file_data = read_data(file)\n",
        "                file_contents.append(file_data)\n",
        "    return file_contents\n",
        "\n",
        "\n",
        "def read_data(file_path):\n",
        "    # Check if the file is a CSV file\n",
        "    if file_path.endswith('.csv'):\n",
        "        return read_csv(file_path)\n",
        "\n",
        "    # Check if the file is a text file\n",
        "    elif file_path.endswith('.txt'):\n",
        "        return read_text(file_path)\n",
        "\n",
        "    # Check if the file is a PDF file\n",
        "    elif file_path.endswith('.pdf'):\n",
        "        return read_pdf(file_path)\n",
        "\n",
        "    # Check if the file is a DOCX file\n",
        "    elif file_path.endswith('.docx'):\n",
        "        return read_docx(file_path)\n",
        "\n",
        "    # Check if the file is a PPTX file\n",
        "    elif file_path.endswith('.pptx'):\n",
        "        return read_pptx(file_path)\n",
        "\n",
        "    # Check if the file is an XLSX file\n",
        "    elif file_path.endswith('.xlsx'):\n",
        "        return read_xlsx(file_path)\n",
        "\n",
        "    # Check if the file is a JSON file\n",
        "    elif file_path.endswith('.json'):\n",
        "        return read_json(file_path)\n",
        "\n",
        "    # Check if the file is an HTML file\n",
        "    elif file_path.endswith('.html'):\n",
        "        return read_html(file_path)\n",
        "\n",
        "    # Check if the file is an XML file\n",
        "    elif file_path.endswith('.xml'):\n",
        "        return read_xml(file_path)\n",
        "\n",
        "    # Check if the file is a ZIP file\n",
        "    elif file_path.endswith('.zip'):\n",
        "        return read_zip(file_path)\n",
        "\n",
        "    # Check if the file is a RAR file\n",
        "    elif file_path.endswith('.rar'):\n",
        "        return read_rar(file_path)\n",
        "\n",
        "    # # Assume it's a web page if it's not a file\n",
        "    # elif os.path.exists(file_path):\n",
        "    #     return read_text(file_path)\n",
        "\n",
        "    # Treat it as a web page if it's a URL\n",
        "    elif file_path.startswith('http'):\n",
        "        return read_web_page(file_path)\n",
        "\n",
        "    # If the file type is unknown, return None\n",
        "    else:\n",
        "        print(\"Unsupported type\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "e5d2c018",
        "tags": [],
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "e5d2c018"
    },
    {
      "cell_type": "code",
      "source": [
        "file_path=''\n",
        "text=read_data(file_path)\n",
        "text"
      ],
      "metadata": {
        "trusted": true,
        "id": "Z4CdVsuiRLDu"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Z4CdVsuiRLDu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning the data"
      ],
      "metadata": {
        "id": "7b3f1d57",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "id": "7b3f1d57"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet as wn\n",
        "stop_words = stopwords.words('english')\n",
        "# arabic_stopwords = stopwords.words('arabic')"
      ],
      "metadata": {
        "id": "a09e9eea",
        "tags": [],
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "a09e9eea"
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_non_ascii(text):\n",
        "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
        "    return text.encode('ascii','ignore').decode()\n",
        "    #return ''.join(char for char in text if char.isalpha() and char.isnumeric() or 'ARABIC' in unicodedata.name(char, ''))\n",
        "\n",
        "def remove_brackets_num(text):\n",
        "    return re.sub(\"\\*?\",\"\",text)\n",
        "\n",
        "def to_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "def replace_numbers(text):\n",
        "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
        "    return re.sub(r'\\d+','',text)\n",
        "\n",
        "def remove_whitespace(text):\n",
        "      return text.strip()\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    punctuation= '''!()[]{};:'\"\\<>/?$%^&*_`~='''\n",
        "    for punc in punctuation:\n",
        "        text=text.replace(punc,\"\")\n",
        "    return text\n",
        "\n",
        "def remove_emails(text):\n",
        "    return re.sub(r'[A-Za-z0-9]*@[A-Za-z]*\\.?[A-Za-z0-9]*', \"\", text)\n",
        "\n",
        "def text2words(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "def remove_stopwords(words,stop_words):\n",
        "    return [word for word in words if word not in stop_words]\n",
        "\n",
        "\n",
        "def normalize_text(text):\n",
        "    text = remove_non_ascii(text)\n",
        "    text= remove_brackets_num(text)\n",
        "    text = to_lowercase(text)\n",
        "    #text=replace_numbers(text)\n",
        "    text= remove_whitespace(text)\n",
        "    text = remove_punctuation(text)\n",
        "    text= remove_emails(text)\n",
        "    words = text2words(text)\n",
        "    #words = remove_stopwords(words, stop_words)\n",
        "\n",
        "    return ' '.join(words)"
      ],
      "metadata": {
        "id": "cb1c452e",
        "tags": [],
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "cb1c452e"
    },
    {
      "cell_type": "code",
      "source": [
        "N_text=normalize_text(text)\n",
        "N_text"
      ],
      "metadata": {
        "trusted": true,
        "id": "pMoY6e33RLD1"
      },
      "execution_count": null,
      "outputs": [],
      "id": "pMoY6e33RLD1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate KeyWords"
      ],
      "metadata": {
        "id": "6d83352b",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "id": "6d83352b"
    },
    {
      "cell_type": "code",
      "source": [
        "from keybert import KeyBERT\n",
        "kw_tool = KeyBERT()"
      ],
      "metadata": {
        "id": "4c066f60",
        "tags": [],
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "4c066f60"
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# def jaccard_similarity(word1, word2):\n",
        "#     set1 = set(word1)\n",
        "#     set2 = set(word2)\n",
        "\n",
        "#     intersection = len(set1.intersection(set2))\n",
        "#     union = len(set1.union(set2))\n",
        "\n",
        "#     similarity = intersection / union if union > 0 else 0\n",
        "#     return similarity\n",
        "\n",
        "def calculate_similarity(sentence1, sentence2):\n",
        "    # Initialize Porter Stemmer\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    # Tokenize and stem the sentences\n",
        "    stemmed_sentence1 = ' '.join([stemmer.stem(word) for word in sentence1.split()])\n",
        "    stemmed_sentence2 = ' '.join([stemmer.stem(word) for word in sentence2.split()])\n",
        "\n",
        "    # Convert the stemmed sentences into vectors\n",
        "    vectorizer = CountVectorizer().fit([stemmed_sentence1, stemmed_sentence2])\n",
        "    vectorized_sentences = vectorizer.transform([stemmed_sentence1, stemmed_sentence2])\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    cosine_sim = cosine_similarity(vectorized_sentences)[0][1]\n",
        "\n",
        "    return cosine_sim\n",
        "\n",
        "\n",
        "def extract_keywords_from_text1(text):\n",
        "    # Extract keywords from the given text\n",
        "    KeyBERT1 = kw_tool.extract_keywords(text, keyphrase_ngram_range=(1,1), top_n=10)\n",
        "    KeyBERT2 = kw_tool.extract_keywords(text, keyphrase_ngram_range=(2,2), top_n=10)\n",
        "\n",
        "    # Combine all extracted keywords\n",
        "    all_keywords = [key[0] for key in KeyBERT1] + \\\n",
        "                   [key[0] for key in KeyBERT2]\n",
        "    # Filter out empty keywords\n",
        "    all_keywords = [keyword for keyword in all_keywords if keyword]\n",
        "\n",
        "    # Filter out very similar keywords\n",
        "    similarity_threshold_between_keywords = 0.4  # Threshold for similarity between keywords\n",
        "    unique_keywords = []\n",
        "    for keyword in all_keywords:\n",
        "        if all(calculate_similarity(keyword, existing_keyword) < similarity_threshold_between_keywords for existing_keyword in unique_keywords):\n",
        "            unique_keywords.append(keyword)\n",
        "\n",
        "    return unique_keywords"
      ],
      "metadata": {
        "id": "c7a1388c",
        "tags": [],
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "c7a1388c"
    },
    {
      "cell_type": "code",
      "source": [
        "unique_keywords1=extract_keywords_from_text1(N_text)\n",
        "unique_keywords1"
      ],
      "metadata": {
        "id": "AZfAieS3RLD5"
      },
      "execution_count": null,
      "outputs": [],
      "id": "AZfAieS3RLD5"
    },
    {
      "cell_type": "code",
      "source": [
        "import pke\n",
        "\n",
        "def extract_keywords_from_text2(text):\n",
        "    # Initialize keyphrase extraction model, here TopicRank\n",
        "    extractor = pke.unsupervised.TopicRank()\n",
        "\n",
        "    # Load the content of the document\n",
        "    extractor.load_document(input=text, language='en')\n",
        "\n",
        "    # Keyphrase candidate selection: in the case of TopicRank: sequences of nouns\n",
        "    # and adjectives (i.e., `(Noun|Adj)*`)\n",
        "    extractor.candidate_selection()\n",
        "\n",
        "    # Candidate weighting: using a random walk algorithm\n",
        "    extractor.candidate_weighting()\n",
        "\n",
        "    # N-best selection, keyphrases contains the 10 highest scored candidates\n",
        "    keyphrases = extractor.get_n_best(n=20)\n",
        "\n",
        "    # Extract keyphrases\n",
        "    keywords = [keyphrase for keyphrase, score in keyphrases]\n",
        "\n",
        "    # Calculate similarity with keywords\n",
        "    unique_keywords = []\n",
        "\n",
        "    # Handling unigrams and bigrams separately\n",
        "    unigrams = [keyphrase for keyphrase in keywords if len(keyphrase.split()) == 1]\n",
        "    bigrams = [keyphrase for keyphrase in keywords if len(keyphrase.split()) == 2]\n",
        "\n",
        "    # Add unigrams to unique_keywords directly\n",
        "    unique_keywords.extend(unigrams)\n",
        "\n",
        "    # Filter bigrams based on similarity with existing keywords\n",
        "    for keyphrase in bigrams:\n",
        "        similarity = calculate_similarity(keyphrase, ' '.join(keywords))\n",
        "        if similarity < 0.4:  # Adjust the similarity threshold as needed\n",
        "            unique_keywords.append(keyphrase)\n",
        "\n",
        "    return unique_keywords"
      ],
      "metadata": {
        "trusted": true,
        "id": "3jrKiUFeRLD6"
      },
      "execution_count": null,
      "outputs": [],
      "id": "3jrKiUFeRLD6"
    },
    {
      "cell_type": "code",
      "source": [
        "unique_keywords2=extract_keywords_from_text2(N_text)\n",
        "unique_keywords2"
      ],
      "metadata": {
        "trusted": true,
        "id": "-9vXtsYtRLD7"
      },
      "execution_count": null,
      "outputs": [],
      "id": "-9vXtsYtRLD7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Distractors"
      ],
      "metadata": {
        "id": "05818b09",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "id": "05818b09"
    },
    {
      "cell_type": "code",
      "source": [
        "from sense2vec import Sense2Vec\n",
        "# load sense2vec vectors\n",
        "s2v = Sense2Vec().from_disk('s2v_old')"
      ],
      "metadata": {
        "id": "86fe680f",
        "tags": [],
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "86fe680f"
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "def sense2vec_get_words(word, s2v):\n",
        "    output = []\n",
        "    word = word.lower()\n",
        "\n",
        "    sense = s2v.get_best_sense(word)\n",
        "    similarity_threshold = 0.3\n",
        "    out = []\n",
        "\n",
        "    if sense is not None:\n",
        "        most_similar = s2v.most_similar(sense, n=20)\n",
        "        for sim in most_similar:\n",
        "            append_word= sim[0].split(\"|\")[0].replace(\"_\", \" \").lower()\n",
        "\n",
        "            # Check similarity with keyword\n",
        "            similarity_keyword = calculate_similarity(word, append_word)\n",
        "            #print(f\"Similarity between '{word}' and '{append_word}': {similarity_keyword}\")\n",
        "\n",
        "            # Check if similarity with keyword is above the threshold\n",
        "            if similarity_keyword >= similarity_threshold:\n",
        "                continue\n",
        "\n",
        "            # Check similarity with existing distractors\n",
        "            similarity_to_existing = [calculate_similarity(append_word, existing_distractor) for existing_distractor in output]\n",
        "\n",
        "            # Check if similarity with any existing distractor is above the threshold\n",
        "            if any(similarity >= similarity_threshold for similarity in similarity_to_existing):\n",
        "                continue\n",
        "\n",
        "            # If the conditions are met, append the word to the list of output\n",
        "            output.append(append_word.title())\n",
        "\n",
        "        out = list(OrderedDict.fromkeys(output))\n",
        "    return out[:3]"
      ],
      "metadata": {
        "trusted": true,
        "id": "3T9Pwa9eRLD9"
      },
      "execution_count": null,
      "outputs": [],
      "id": "3T9Pwa9eRLD9"
    },
    {
      "cell_type": "code",
      "source": [
        "for word in unique_keywords:\n",
        "    existing_distractor=sense2vec_get_words(word, s2v)\n",
        "    print(word , existing_distractor)"
      ],
      "metadata": {
        "trusted": true,
        "id": "qIBD-g_0RLD_"
      },
      "execution_count": null,
      "outputs": [],
      "id": "qIBD-g_0RLD_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### At Inference Time"
      ],
      "metadata": {
        "id": "z8pxtjdmRLEA"
      },
      "id": "z8pxtjdmRLEA"
    },
    {
      "cell_type": "code",
      "source": [
        "#general question generation model\n",
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "HUGGING_FACE_USER_NAME=''\n",
        "model_name=''\n",
        "peft_model_id = f\"{HUGGING_FACE_USER_NAME}/{model_name}\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=False, device_map='auto')\n",
        "G_tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "# Load the Lora model\n",
        "G_model = PeftModel.from_pretrained(model, peft_model_id)"
      ],
      "metadata": {
        "trusted": true,
        "id": "7GrEOlC7RLEA"
      },
      "execution_count": null,
      "outputs": [],
      "id": "7GrEOlC7RLEA"
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_questions(context, answer, distractors):\n",
        "    device = next(G_model.parameters()).device\n",
        "    input_text = f\"Given the context '{context}' and the answer '{answer}' , what question can be asked?\"\n",
        "    encoding = G_tokenizer.encode_plus(input_text, padding=True,truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    output_tokens = G_model.generate(**encoding, early_stopping=True, num_beams=5, num_return_sequences=1, no_repeat_ngram_size=2, max_length=200)\n",
        "    question = G_tokenizer.decode(output_tokens[0], skip_special_tokens=True).replace(\"question:\", \"\").strip()\n",
        "    return question"
      ],
      "metadata": {
        "trusted": true,
        "id": "QQbt6nc6RLEB"
      },
      "execution_count": null,
      "outputs": [],
      "id": "QQbt6nc6RLEB"
    },
    {
      "cell_type": "code",
      "source": [
        "context_printed = False  # Flag to keep track of whether the context has been printed\n",
        "\n",
        "# Iterate over unique keywords and generate questions for keywords with distractor lists\n",
        "for word in unique_keywords:\n",
        "    existing_distractors = sense2vec_get_words(word, s2v)\n",
        "    if existing_distractors:\n",
        "        if not context_printed:\n",
        "            print(\"Context:\", N_text)\n",
        "            print()\n",
        "            context_printed = True  # Set the flag to True after printing the context\n",
        "\n",
        "        context = N_text\n",
        "        answer = word\n",
        "        distractors = existing_distractors\n",
        "        question = generate_questions(context, answer, existing_distractors)\n",
        "\n",
        "        print(\"Answer:\", answer)\n",
        "        print(\"Distractors:\", existing_distractors)\n",
        "        print(\"Generated Question:\", question)\n",
        "        print()"
      ],
      "metadata": {
        "trusted": true,
        "id": "9ytad3lARLEC"
      },
      "execution_count": null,
      "outputs": [],
      "id": "9ytad3lARLEC"
    },
    {
      "cell_type": "code",
      "source": [
        "def process_and_generate_questions(file_path):\n",
        "    # Read the data from the file\n",
        "    example = read_data(file_path)\n",
        "    N_text_file, keywords, keyword_question_distractors = \"\", [], []\n",
        "\n",
        "    # Check if example is not None and not empty\n",
        "    if example is not None and example.strip():\n",
        "        # Remove empty lines between sentences\n",
        "        cleaned_text = '\\n'.join(line.strip() for line in example.split('\\n') if line.strip())\n",
        "\n",
        "        # Concatenate lines and separate them with a period\n",
        "        concatenated_text = '.'.join(cleaned_text.split('\\n'))\n",
        "\n",
        "        # Tokenize the text using G_tokenizer\n",
        "        tokens = G_tokenizer.tokenize(concatenated_text)\n",
        "\n",
        "        # Check if the number of tokens is less than 1024\n",
        "        if len(tokens) < 1024: # or 512 for google-flan-t5\n",
        "            # Normalize the concatenated text\n",
        "            N_text_file = normalize_text(concatenated_text)\n",
        "\n",
        "            # Check if N_text_file has text\n",
        "            if N_text_file:\n",
        "                keywords = extract_keywords_from_text(N_text_file)\n",
        "                if keywords:\n",
        "                    for word in keywords:\n",
        "                        current_distractors = sense2vec_get_words(word, s2v)\n",
        "                        if current_distractors:\n",
        "                            question = generate_questions(N_text_file, word)\n",
        "                            keyword_question_distractors.append((word, current_distractors, question))\n",
        "                else:\n",
        "                    print(\"No keywords generated.\")\n",
        "            else:\n",
        "                print(\"No text available.\")\n",
        "        else:\n",
        "            print(\"The tokenized text has more than 1024 tokens.\")\n",
        "    else:\n",
        "        print(\"The file is empty or large.\")\n",
        "        # Handle the case where the file is empty or cannot be loaded\n",
        "\n",
        "    result = [(N_text_file, keyword_question_distractors)]\n",
        "\n",
        "    return result\n",
        "\n",
        "# Example usage:\n",
        "file_path = \"\"\n",
        "Total_List = process_and_generate_questions(file_path)\n",
        "\n",
        "# Print the context, keyword, question, and distractors\n",
        "for context, keyword_question_distractors in Total_List:\n",
        "    if context:\n",
        "        print(f\"context: {context}\")\n",
        "        print()\n",
        "        for keyword, distractors, question in keyword_question_distractors:\n",
        "            print(f\"Keyword: {keyword}\")\n",
        "            print(f\"Question: {question}\")\n",
        "            print(f\"Distractors: {distractors}\")\n",
        "            print()"
      ],
      "metadata": {
        "trusted": true,
        "id": "4aFeTnPXRLED"
      },
      "execution_count": null,
      "outputs": [],
      "id": "4aFeTnPXRLED"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ###########################################"
      ],
      "metadata": {
        "id": "22BD4ZECRLEF"
      },
      "id": "22BD4ZECRLEF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For Training Time"
      ],
      "metadata": {
        "id": "exLV1HPzRLEG"
      },
      "id": "exLV1HPzRLEG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### loading the model and the tokenizer"
      ],
      "metadata": {
        "id": "f30fd042",
        "tags": []
      },
      "id": "f30fd042"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
        "\n",
        "model_name_or_path = \"facebook/bart-base\" # google/flan-t5-base\n",
        "\n",
        "# Define the quantization configuration with 4-bit\n",
        "# quantization_config = BitsAndBytesConfig(bit_width=4, bnb_4bit_compute_type=\"torch.float16\")\n",
        "\n",
        "# Load the model with the specified quantization configuration\n",
        "G_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    torch_dtype=torch.float32,\n",
        "    device_map='auto',\n",
        ")\n",
        "G_tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)"
      ],
      "metadata": {
        "id": "4c008ac5-d6ca-4569-a4d7-d6bf53503205",
        "tags": [],
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "4c008ac5-d6ca-4569-a4d7-d6bf53503205"
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "Lora_config = LoraConfig(\n",
        "    r=18,\n",
        "    lora_alpha=12,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"], # or q and v for other models\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM\n",
        ")\n",
        "\n",
        "L_model = get_peft_model(G_model, Lora_config)\n",
        "print(L_model.print_trainable_parameters())"
      ],
      "metadata": {
        "id": "dba4b9f7",
        "tags": [],
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "dba4b9f7"
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, load_dataset\n",
        "\n",
        "#for diff english data\n",
        "en = (\n",
        "    load_dataset(\"mou3az/Question-Answering-Generation-Choices\", split=\"train\")\n",
        "    .filter(lambda example: len(example[\"context\"]) < 951)\n",
        ")\n",
        "# # Split the filtered dataset into training and validation sets\n",
        "en_train = en.select(range(19500))  # Select first 15,000 examples for training\n",
        "en_validation = en.select(range(19500, 20901))  # Select next 2,500 examples for validation"
      ],
      "metadata": {
        "id": "55751b0d",
        "tags": [],
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "55751b0d"
    },
    {
      "cell_type": "code",
      "source": [
        "#For General data\n",
        "# import ast\n",
        "def create_prompt1(context, answer):\n",
        "    input_text = f\"Given the context '{context}' and the answer '{answer}' , what question can be asked?\"\n",
        "    return input_text\n",
        "\n",
        "def create_prompt2(question):\n",
        "    output_text = f\"question: {question}\"\n",
        "    return output_text"
      ],
      "metadata": {
        "id": "85d18ac3",
        "tags": [],
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "85d18ac3"
    },
    {
      "cell_type": "code",
      "source": [
        "#for english data\n",
        "en_train_data = en_train.map(lambda samples: G_tokenizer.encode_plus(create_prompt1(samples['context'], samples['answer']), padding=True), remove_columns=[\"context\", \"answer\", \"question\",\"distractors\"])\n",
        "en_validation_data = en_validation.map(lambda samples: G_tokenizer.encode_plus(create_prompt1(samples['context'], samples['answer']), padding=True), remove_columns=[\"context\", \"answer\", \"question\",'distractors'])\n",
        "en_question_Tdata = en_train.map(lambda samples: G_tokenizer.encode_plus(create_prompt2(samples['question']), padding=True), remove_columns=[\"context\", \"answer\", \"question\",'distractors'])[\"input_ids\"]\n",
        "en_question_Vdata = en_validation.map(lambda samples: G_tokenizer.encode_plus(create_prompt2(samples['question']), padding=True), remove_columns=[\"context\", \"answer\", \"question\",'distractors'])[\"input_ids\"]\n",
        "en_train_data=en_train_data.add_column(\"labels\", en_question_Tdata)\n",
        "en_validation_data=en_validation_data.add_column(\"labels\", en_question_Vdata)"
      ],
      "metadata": {
        "id": "c60d0004",
        "tags": [],
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "c60d0004"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, EarlyStoppingCallback, Seq2SeqTrainer\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "      gradient_accumulation_steps=10,\n",
        "      per_device_train_batch_size=45,\n",
        "      per_device_eval_batch_size=45,\n",
        "      # save_steps=2,\n",
        "      eval_steps=150,\n",
        "      warmup_steps=150,\n",
        "      logging_steps=150,\n",
        "      weight_decay=0.05,\n",
        "      # save_total_limit=5,\n",
        "      learning_rate=3e-3,\n",
        "      max_steps=3000,\n",
        "      # num_train_epochs=2,\n",
        "      # load_best_model_at_end=True,\n",
        "      # gradient_checkpointing=True,\n",
        "      lr_scheduler_type=\"linear\",\n",
        "      do_train=True,\n",
        "      do_eval=True,\n",
        "      # fp16=False,\n",
        "      report_to=\"all\",\n",
        "      log_level=\"debug\",\n",
        "      logging_dir='./logs',\n",
        "      output_dir='./outputs',\n",
        "      label_names=[\"labels\"],\n",
        "      evaluation_strategy=\"steps\",\n",
        "      # metric_for_best_model=\"eval_loss\",\n",
        "    )\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=L_model,\n",
        "    args=training_args,\n",
        "    tokenizer=G_tokenizer,\n",
        "    train_dataset=en_train_data,\n",
        "    eval_dataset=en_validation_data,\n",
        "    # callbacks=[EarlyStoppingCallback(2, 1.0)],\n",
        "    data_collator=DataCollatorForSeq2Seq(G_tokenizer,label_pad_token_id=-100),\n",
        ")\n",
        "\n",
        "# Additional configuration\n",
        "L_model.config.use_cache = False\n",
        "torch.cuda.empty_cache()\n",
        "# L_model.config.bnb_8bit_compute_type = \"torch.float16\"\n",
        "\n",
        "# Start training\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "16c9fd76",
        "tags": [],
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "16c9fd76"
    },
    {
      "cell_type": "code",
      "source": [
        "# to hugging face\n",
        "model_name = \"\"\n",
        "HUGGING_FACE_USER_NAME = \"\"\n",
        "\n",
        "L_model.push_to_hub(f\"{HUGGING_FACE_USER_NAME}/{model_name}\", token='')"
      ],
      "metadata": {
        "id": "1d90c56e",
        "tags": [],
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "1d90c56e"
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model checkpoint\n",
        "L_model.save_pretrained(\"\")\n",
        "# Create a zip archive\n",
        "!zip -r saved_model.zip VF"
      ],
      "metadata": {
        "trusted": true,
        "id": "iv_I98LeRLEN"
      },
      "execution_count": null,
      "outputs": [],
      "id": "iv_I98LeRLEN"
    },
    {
      "cell_type": "code",
      "source": [
        "#general question generation model\n",
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "HUGGING_FACE_USER_NAME=''\n",
        "model_name=''\n",
        "peft_model_id = f\"{HUGGING_FACE_USER_NAME}/{model_name}\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=False, device_map='auto')\n",
        "G_tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "# Load the Lora model\n",
        "L_model = PeftModel.from_pretrained(model, peft_model_id)"
      ],
      "metadata": {
        "trusted": true,
        "id": "fXCtjA79RLEO"
      },
      "execution_count": null,
      "outputs": [],
      "id": "fXCtjA79RLEO"
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_questions(context, answer):\n",
        "    device = next(L_model.parameters()).device\n",
        "    input_text = f\"Given the context '{context}' and the answer '{answer}', what question can be asked?\"\n",
        "    encoding = G_tokenizer.encode_plus(input_text, padding=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    output_tokens = L_model.generate(\n",
        "        **encoding,\n",
        "        early_stopping=True,\n",
        "        do_sample= True,\n",
        "        num_beams=5,\n",
        "        num_return_sequences=1,\n",
        "        no_repeat_ngram_size=2,\n",
        "        max_length=256,\n",
        "        temperature=0.6,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.2\n",
        "    )\n",
        "    question = G_tokenizer.decode(output_tokens[0], skip_special_tokens=True).replace(\"question :\", \"\").strip()\n",
        "    return question"
      ],
      "metadata": {
        "trusted": true,
        "id": "Z2V8EyKzRLEP"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Z2V8EyKzRLEP"
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from rouge import Rouge\n",
        "from fuzzywuzzy import fuzz\n",
        "from bert_score import score\n",
        "\n",
        "\n",
        "def calculate_bleu_scores(references, predictions):\n",
        "    return corpus_bleu([[ref.split()] for ref in references], [pred.split() for pred in predictions])\n",
        "\n",
        "def calculate_rouge_scores(references, predictions):\n",
        "    rouge = Rouge()\n",
        "    rouge_scores = rouge.get_scores(predictions, references, avg=True)\n",
        "    return rouge_scores\n",
        "\n",
        "def calculate_accuracy(references, predictions):\n",
        "    accuracies = [fuzz.token_sort_ratio(ref, pred) / 100.0 for ref, pred in zip(references, predictions)]\n",
        "    return sum(accuracies) / len(accuracies)\n",
        "\n",
        "def calculate_bert_score(references, predictions):\n",
        "    P, R, F1 = score(predictions, references, lang='en', verbose=False)\n",
        "    return F1.mean().item()\n",
        "\n",
        "def evaluate(dataset):\n",
        "    references = [sample['question'] for sample in dataset]\n",
        "    predictions = [generate_questions(sample['context'], sample['answer']) for sample in dataset]  # Assuming 'generate_questions' generates model's output\n",
        "\n",
        "    bleu_score = calculate_bleu_scores(references, predictions)\n",
        "    rouge_scores = calculate_rouge_scores(references, predictions)\n",
        "    accuracy = calculate_accuracy(references, predictions)\n",
        "    bert_score = calculate_bert_score(references, predictions)\n",
        "\n",
        "    print(\"Overall Accuracy:\", accuracy)\n",
        "    print(\"Overall BLEU Score:\", bleu_score)\n",
        "    print(\"Overall ROUGE Score:\", rouge_scores)\n",
        "    print(\"Overall BERTScore:\", bert_score)\n",
        "\n",
        "    return accuracy, bleu_score, rouge_scores, bert_score\n",
        "\n",
        "# Assuming 'en_validation' is your dataset\n",
        "accuracy, bleu_score, rouge_scores, bert_score = evaluate(en_validation)"
      ],
      "metadata": {
        "trusted": true,
        "id": "_khWtHr9RLEQ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "_khWtHr9RLEQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ###########################################"
      ],
      "metadata": {
        "id": "cLu71ILVRb9_"
      },
      "id": "cLu71ILVRb9_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other Method"
      ],
      "metadata": {
        "id": "h0qGw8h7RSqE"
      },
      "id": "h0qGw8h7RSqE"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub flask"
      ],
      "metadata": {
        "id": "LCO5Yy8kR1XD"
      },
      "id": "LCO5Yy8kR1XD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from huggingface_hub import InferenceClient\n",
        "from requests.exceptions import RequestException\n",
        "import os\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"your_hf_token\"\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "\n",
        "client = InferenceClient(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
        "\n",
        "def system_instructions(context):\n",
        "    return f\"\"\"<s> [INST] Your are a great teacher and your task is to create 6 questions with answer and 4 choices based on the following context:\\n\\n{context}\\n\\n. Each example should be like this\n",
        "    Question: \"\"\n",
        "    Choices:\n",
        "    A): \"\"\n",
        "    B): \"\"\n",
        "    C): \"\"\n",
        "    D): \"\"\n",
        "    Answer: \"A or B or C or D according to the right answer\"\n",
        "    Explanation: \"\"\n",
        "    \\n\n",
        "    [/INST]\n",
        "    \"\"\"\n",
        "\n",
        "def generate_quiz(context):\n",
        "    formatted_prompt = system_instructions(context)\n",
        "\n",
        "    generate_kwargs = dict(\n",
        "            temperature=0.1,\n",
        "            max_new_tokens=2048,\n",
        "            top_p=0.95,\n",
        "            repetition_penalty=1.0,\n",
        "            do_sample=True,\n",
        "            seed=42,)\n",
        "\n",
        "    try:\n",
        "        response = client.text_generation(\n",
        "            formatted_prompt,\n",
        "            **generate_kwargs,\n",
        "            stream=False,\n",
        "            details=False,\n",
        "            return_full_text=False,\n",
        "        )\n",
        "        return response\n",
        "\n",
        "    except (RequestException, SystemExit) as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
        "def generate_quiz_page():\n",
        "    if request.method == \"POST\":\n",
        "        if request.content_type == 'application/json':\n",
        "            data = request.get_json()\n",
        "            context = data.get(\"context\")\n",
        "            if context is None or context.strip() == \"\":\n",
        "                return jsonify({\"error\": \"Missing or empty 'context' parameter\"}), 400\n",
        "        else:\n",
        "            context = request.form.get(\"context\")\n",
        "            if context is None or context.strip() == \"\":\n",
        "                return jsonify({\"error\": \"Missing or empty 'context' parameter\"}), 400\n",
        "\n",
        "        response = generate_quiz(context)\n",
        "\n",
        "        if request.content_type == 'application/json':\n",
        "            return jsonify(response)\n",
        "\n",
        "\n",
        "        quiz_html = f\"\"\"\n",
        "        <!DOCTYPE html>\n",
        "        <html lang=\"en\">\n",
        "        <head>\n",
        "            <meta charset=\"UTF-8\">\n",
        "            <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "            <title>Generated Quiz</title>\n",
        "            <style>\n",
        "                body {{\n",
        "                    font-family: Arial, sans-serif;\n",
        "                    background-color: #f0f0f0; /* Light grey background */\n",
        "                    color: #333; /* Dark grey text color */\n",
        "                    margin: 20px;\n",
        "                }}\n",
        "                h2 {{\n",
        "                    background-color: #ffc107; /* Yellow background for heading */\n",
        "                    padding: 10px;\n",
        "                    border-radius: 5px;\n",
        "                }}\n",
        "                form {{\n",
        "                    display: none; /* Hide the form after generating quiz */\n",
        "                }}\n",
        "                .quiz-container {{\n",
        "                    background-color: #f0f0f0; /* Light grey background */\n",
        "                    padding: 20px;\n",
        "                    border-radius: 5px;\n",
        "                    box-shadow: 0 0 10px rgba(0, 0, 0, 0.1); /* Light shadow effect */\n",
        "                    margin-bottom: 20px;\n",
        "                }}\n",
        "                .quiz-container pre {{\n",
        "                    white-space: pre-wrap; /* Preserve line breaks in quiz response */\n",
        "                }}\n",
        "                .generate-link {{\n",
        "                    display: inline-block; /* Make the link inline-block */\n",
        "                    margin-top: 10px;\n",
        "                    border: 2px solid transparent; /* Transparent border initially */\n",
        "                    padding: 5px 10px; /* Padding for spacing inside the border */\n",
        "                    text-decoration: none; /* Remove underline */\n",
        "                    color: #ffc107; /* Yellow text color */\n",
        "                    font-weight: bold; /* Bold text */\n",
        "                    border-radius: 4px;\n",
        "                    background-color: #333; /* Dark grey background color */\n",
        "                }}\n",
        "                .generate-link:hover {{\n",
        "                    text-decoration: none; /* Remove underline on hover */\n",
        "                    background-color: #555; /* Darker grey background color on hover */\n",
        "                }}\n",
        "                a {{\n",
        "                    color: #ffc107; /* Yellow text color for links */\n",
        "                    text-decoration: none;\n",
        "                }}\n",
        "                a:hover {{\n",
        "                    text-decoration: underline; /* Underline on hover */\n",
        "                }}\n",
        "                textarea {{\n",
        "                    width: 100%;\n",
        "                    padding: 10px;\n",
        "                    border: 1px solid #ccc;\n",
        "                    border-radius: 4px;\n",
        "                    resize: vertical; /* Allow vertical resizing of textarea */\n",
        "                    min-height: 150px; /* Minimum height for textarea */\n",
        "                }}\n",
        "                input[type=\"submit\"] {{\n",
        "                    background-color: #ffc107; /* Yellow background for submit button */\n",
        "                    color: #333; /* Dark grey text color */\n",
        "                    border: none;\n",
        "                    padding: 10px 20px;\n",
        "                    cursor: pointer;\n",
        "                    border-radius: 4px;\n",
        "                }}\n",
        "                input[type=\"submit\"]:hover {{\n",
        "                    background-color: #ffca28; /* Lighter yellow on hover */\n",
        "                }}\n",
        "            </style>\n",
        "        </head>\n",
        "        <body>\n",
        "            <div class=\"quiz-container\">\n",
        "                <h2>Generated Quiz</h2>\n",
        "                <pre>{response}</pre>\n",
        "                <a href=\"/\" class=\"generate-link\">Back to generate another quiz</a>\n",
        "            </div>\n",
        "        </body>\n",
        "        </html>\n",
        "        \"\"\"\n",
        "\n",
        "        return quiz_html\n",
        "\n",
        "    # Default GET request handling\n",
        "    default_html = \"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html lang=\"en\">\n",
        "    <head>\n",
        "        <meta charset=\"UTF-8\">\n",
        "        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "        <title>Generate Quiz</title>\n",
        "        <style>\n",
        "            body {\n",
        "                font-family: Arial, sans-serif;\n",
        "                background-color: #f0f0f0; /* Light grey background */\n",
        "                color: #333; /* Dark grey text color */\n",
        "                margin: 20px;\n",
        "            }\n",
        "            h2 {\n",
        "                background-color: #ffc107; /* Yellow background for heading */\n",
        "                padding: 10px;\n",
        "                border-radius: 5px;\n",
        "            }\n",
        "            form {\n",
        "                background-color: #fff; /* White background for form */\n",
        "                padding: 20px;\n",
        "                border-radius: 5px;\n",
        "                box-shadow: 0 0 10px rgba(0, 0, 0, 0.1); /* Light shadow effect */\n",
        "                margin-bottom: 20px;\n",
        "            }\n",
        "            label {\n",
        "                display: block;\n",
        "                margin-bottom: 10px;\n",
        "            }\n",
        "            textarea {\n",
        "                width: 98%;\n",
        "                padding: 10px;\n",
        "                border: 2px solid #ffc107; /* Yellow border */\n",
        "                border-radius: 4px;\n",
        "                resize: vertical; /* Allow vertical resizing of textarea */\n",
        "                min-height: 150px; /* Minimum height for textarea */\n",
        "                background-color: #f0f0f0; /* Light grey background */\n",
        "            }\n",
        "            input[type=\"submit\"] {\n",
        "                background-color: #ffc107; /* Yellow background for submit button */\n",
        "                color: #333; /* Dark grey text color */\n",
        "                border: none;\n",
        "                padding: 10px 20px;\n",
        "                cursor: pointer;\n",
        "                border-radius: 4px;\n",
        "            }\n",
        "            input[type=\"submit\"]:hover {\n",
        "                background-color: #ffca28; /* Lighter yellow on hover */\n",
        "            }\n",
        "        </style>\n",
        "    </head>\n",
        "    <body>\n",
        "        <h2>Generate Quiz</h2>\n",
        "        <form action=\"/\" method=\"post\">\n",
        "            <label for=\"context\">Context:</label><br>\n",
        "            <textarea id=\"context\" name=\"context\" rows=\"20\" required></textarea><br><br>\n",
        "            <input type=\"submit\" value=\"Generate Quiz\">\n",
        "        </form>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "\n",
        "    return default_html\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(debug=True)"
      ],
      "metadata": {
        "id": "hW564iLRRWnK"
      },
      "id": "hW564iLRRWnK",
      "execution_count": null,
      "outputs": []
    }
  ]
}