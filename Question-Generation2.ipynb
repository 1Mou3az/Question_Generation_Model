{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"collapsed_sections":["f24b75b3","cc94a774","7b3f1d57","6d83352b","05818b09"]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7749513,"sourceType":"datasetVersion","datasetId":4530578},{"sourceId":7770645,"sourceType":"datasetVersion","datasetId":4545907},{"sourceId":7771818,"sourceType":"datasetVersion","datasetId":4546679},{"sourceId":7792920,"sourceType":"datasetVersion","datasetId":4561912},{"sourceId":7795842,"sourceType":"datasetVersion","datasetId":4564047},{"sourceId":7796098,"sourceType":"datasetVersion","datasetId":4564227}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"6f3ec6892726467ebe3fae7f102fae16":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_22018e6ad60c48dfa066b054320081e6","IPY_MODEL_b2640b2b37be445e937ca6eea99087e7","IPY_MODEL_bf7d55d6228949a9a2363050553a79ce"],"layout":"IPY_MODEL_63bfa208041a41bb9ad6068bed803a45"}},"22018e6ad60c48dfa066b054320081e6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4dafa2ee5faf4509912f7a68744d5a90","placeholder":"​","style":"IPY_MODEL_57a6ed1d171840c1af8383198f0ec4f1","value":"Map: 100%"}},"b2640b2b37be445e937ca6eea99087e7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3b411705e7947d7a534cc67e75a7b5e","max":999,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b69da5d4e1384585a28d6fefef280137","value":999}},"bf7d55d6228949a9a2363050553a79ce":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aaf7271fef0b4032901703c97002c610","placeholder":"​","style":"IPY_MODEL_8aea77791d3340c6ba4e236739b5a0c3","value":" 999/999 [00:00&lt;00:00, 1656.49 examples/s]"}},"63bfa208041a41bb9ad6068bed803a45":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4dafa2ee5faf4509912f7a68744d5a90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57a6ed1d171840c1af8383198f0ec4f1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d3b411705e7947d7a534cc67e75a7b5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b69da5d4e1384585a28d6fefef280137":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"aaf7271fef0b4032901703c97002c610":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8aea77791d3340c6ba4e236739b5a0c3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"# for training\n!pip install -q -U git+https://github.com/huggingface/transformers.git\n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install fuzzywuzzy python-Levenshtein\n!pip install bitsandbytes==0.41.3\n!pip install -q -U sentencepiece\n!pip install -q -U accelerate\n!pip install -q -U datasets\n!pip install nltk==3.5.0\n!pip install bert-score\n!pip install rouge\n#for inference time\n!pip install xlrd\n!pip install PyPDF2\n!pip install keybert\n!pip install rarfile\n!pip install openpyxl\n!pip install python-pptx\n!pip install python-docx\n!pip install keybert[use]\n!pip install keybert[spacy]\n!pip install keybert[flair]\n!pip install keybert[gensim]\n!pip install sense2vec==2.0.1\n!python -m spacy download en_core_web_sm\n!pip install git+https://github.com/boudinfl/pke.git\n!wget https://github.com/explosion/sense2vec/releases/download/v1.0.0/s2v_reddit_2015_md.tar.gz\n!tar -xvf  s2v_reddit_2015_md.tar.gz","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ac09373e","outputId":"2ceefb62-3932-4022-a845-217517132e47","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n# if torch.cuda.is_available()==True :\n#     os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"","metadata":{"id":"c06f6c13","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# #########################################","metadata":{}},{"cell_type":"markdown","source":"# For Inference Time","metadata":{}},{"cell_type":"markdown","source":"### dealing with the data","metadata":{"id":"cc94a774","jp-MarkdownHeadingCollapsed":true,"tags":[]}},{"cell_type":"code","source":"import PyPDF2\nimport requests\nfrom bs4 import BeautifulSoup\nimport csv\nimport zipfile\nimport json\nimport xml.etree.ElementTree as ET\nimport docx\nimport pptx\nimport rarfile\nimport xlrd  # For XLS files\nimport openpyxl  # For XLSX files\nimport shutil  # For extracting ZIP files\n\ndef read_csv(file_path):\n    with open(file_path, 'r', newline='') as csvfile:\n        csv_reader = csv.reader(csvfile)\n        csv_data = [row for row in csv_reader]\n    return csv_data\n\ndef read_text(file_path):\n    with open(file_path, 'r') as f:\n        text_data = f.read()\n    return text_data\n\ndef read_pdf(file_path):\n    pdf_file = open(file_path, 'rb')\n    pdf_reader = PyPDF2.PdfReader(pdf_file)\n    text_data = []\n    for page in pdf_reader.pages:\n        text_data.append(page.extract_text())\n    return text_data\n\ndef read_web_page(url):\n    result = requests.get(url)\n    src = result.content\n    soup = BeautifulSoup(src, 'html.parser')\n    text_data = ''\n    for p in soup.find_all('p'):\n        text_data += p.get_text() + '\\n'\n    return text_data\n\ndef read_docx(file_path):\n    doc = docx.Document(file_path)\n    text_data = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n    return text_data\n\ndef read_pptx(file_path):\n    ppt = pptx.Presentation(file_path)\n    text_data = ''\n    for slide in ppt.slides:\n        for shape in slide.shapes:\n            if hasattr(shape, \"text\"):\n                text_data += shape.text + '\\n'\n    return text_data\n\ndef read_xlsx(file_path):\n    workbook = openpyxl.load_workbook(file_path)\n    sheet = workbook.active\n    text_data = ''\n    for row in sheet.iter_rows(values_only=True):\n        text_data += ' '.join([str(cell) for cell in row if cell is not None]) + '\\n'\n    return text_data\n\n\ndef read_json(file_path):\n    with open(file_path, 'r') as f:\n        json_data = json.load(f)\n    return json_data\n\ndef read_html(file_path):\n    with open(file_path, 'r') as f:\n        html_data = f.read()\n    return html_data\n\ndef read_xml(file_path):\n    tree = ET.parse(file_path)\n    root = tree.getroot()\n    return ET.tostring(root, encoding='unicode')\n\ndef read_zip(file_path):\n    file_contents = []\n    with zipfile.ZipFile(file_path, 'r') as zip_ref:\n        for file_info in zip_ref.infolist():\n            with zip_ref.open(file_info) as file:\n                # Call read_data to handle reading and processing the file contents\n                file_data = read_data(file)\n                file_contents.append(file_data)\n    return file_contents\n\ndef read_rar(file_path):\n    file_contents = []\n    with rarfile.RarFile(file_path, 'r') as rar_ref:\n        for rar_info in rar_ref.infolist():\n            with rar_ref.open(rar_info) as file:\n                # Call read_data to handle reading and processing the file contents\n                file_data = read_data(file)\n                file_contents.append(file_data)\n    return file_contents\n\n\ndef read_data(file_path):\n    # Check if the file is a CSV file\n    if file_path.endswith('.csv'):\n        return read_csv(file_path)\n\n    # Check if the file is a text file\n    elif file_path.endswith('.txt'):\n        return read_text(file_path)\n\n    # Check if the file is a PDF file\n    elif file_path.endswith('.pdf'):\n        return read_pdf(file_path)\n\n    # Check if the file is a DOCX file\n    elif file_path.endswith('.docx'):\n        return read_docx(file_path)\n\n    # Check if the file is a PPTX file\n    elif file_path.endswith('.pptx'):\n        return read_pptx(file_path)\n\n    # Check if the file is an XLSX file\n    elif file_path.endswith('.xlsx'):\n        return read_xlsx(file_path)\n\n    # Check if the file is a JSON file\n    elif file_path.endswith('.json'):\n        return read_json(file_path)\n\n    # Check if the file is an HTML file\n    elif file_path.endswith('.html'):\n        return read_html(file_path)\n\n    # Check if the file is an XML file\n    elif file_path.endswith('.xml'):\n        return read_xml(file_path)\n\n    # Check if the file is a ZIP file\n    elif file_path.endswith('.zip'):\n        return read_zip(file_path)\n\n    # Check if the file is a RAR file\n    elif file_path.endswith('.rar'):\n        return read_rar(file_path)\n\n    # # Assume it's a web page if it's not a file\n    # elif os.path.exists(file_path):\n    #     return read_text(file_path)\n\n    # Treat it as a web page if it's a URL\n    elif file_path.startswith('http'):\n        return read_web_page(file_path)\n\n    # If the file type is unknown, return None\n    else:\n        print(\"Unsupported type\")\n        return None","metadata":{"id":"e5d2c018","outputId":"2b9e6a2e-f656-4d26-c394-711a33afc344","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_path=''\ntext=read_data(file_path)\ntext","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cleaning the data","metadata":{"id":"7b3f1d57","jp-MarkdownHeadingCollapsed":true,"tags":[]}},{"cell_type":"code","source":"import re\nimport string\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('omw-1.4')\nnltk.download('wordnet')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet as wn\nstop_words = stopwords.words('english')\n# arabic_stopwords = stopwords.words('arabic')","metadata":{"id":"a09e9eea","outputId":"7e8f2529-64d8-4163-8421-2afd1d226078","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_non_ascii(text):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    return text.encode('ascii','ignore').decode()\n    #return ''.join(char for char in text if char.isalpha() and char.isnumeric() or 'ARABIC' in unicodedata.name(char, ''))\n\ndef remove_brackets_num(text):\n    return re.sub(\"\\*?\",\"\",text)\n\ndef to_lowercase(text):\n    return text.lower()\n\ndef replace_numbers(text):\n    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n    return re.sub(r'\\d+','',text)\n\ndef remove_whitespace(text):\n      return text.strip()\n\ndef remove_punctuation(text):\n    punctuation= '''!()[]{};:'\"\\<>/?$%^&*_`~='''\n    for punc in punctuation:\n        text=text.replace(punc,\"\")\n    return text\n\ndef remove_emails(text):\n    return re.sub(r'[A-Za-z0-9]*@[A-Za-z]*\\.?[A-Za-z0-9]*', \"\", text)\n\ndef text2words(text):\n    return word_tokenize(text)\n\ndef remove_stopwords(words,stop_words):\n    return [word for word in words if word not in stop_words]\n\n\ndef normalize_text(text):\n    text = remove_non_ascii(text)\n    text= remove_brackets_num(text)\n    text = to_lowercase(text)\n    #text=replace_numbers(text)\n    text= remove_whitespace(text)\n    text = remove_punctuation(text)\n    text= remove_emails(text)\n    words = text2words(text)\n    #words = remove_stopwords(words, stop_words)\n\n    return ' '.join(words)","metadata":{"id":"cb1c452e","outputId":"416f1537-7f54-4f58-c069-98b3c51dc4b1","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_text=normalize_text(text)\nN_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generate KeyWords","metadata":{"id":"6d83352b","jp-MarkdownHeadingCollapsed":true,"tags":[]}},{"cell_type":"code","source":"from keybert import KeyBERT\nkw_tool = KeyBERT()","metadata":{"colab":{"referenced_widgets":["df9e50b5975140d89e3887fd3b19043b","c05eaf0feb09443f94d6d14e830a3674","777fbe69d66549fd86336383227a514c","d02adbccf865442c9def41fc161de68e","82964ee98852448fba330e0f2edccafb","47a5dba3bcb248a390cfb89b2b1f777e","3812140ada794c62bdab1f02c09d1366","9bc760c1d55243689d9b105af8f660bf","94e5edd81666435da1eefadf353e4c09","012b28ce058b4501a933eed641ad06c0","17577032c6bf4b6cb72e8c840f34e8fb"]},"id":"4c066f60","outputId":"ce3fde9e-945b-4ee4-c13d-93fa89482689","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.stem import PorterStemmer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# def jaccard_similarity(word1, word2):\n#     set1 = set(word1)\n#     set2 = set(word2)\n\n#     intersection = len(set1.intersection(set2))\n#     union = len(set1.union(set2))\n\n#     similarity = intersection / union if union > 0 else 0\n#     return similarity\n\ndef calculate_similarity(sentence1, sentence2):\n    # Initialize Porter Stemmer\n    stemmer = PorterStemmer()\n\n    # Tokenize and stem the sentences\n    stemmed_sentence1 = ' '.join([stemmer.stem(word) for word in sentence1.split()])\n    stemmed_sentence2 = ' '.join([stemmer.stem(word) for word in sentence2.split()])\n\n    # Convert the stemmed sentences into vectors\n    vectorizer = CountVectorizer().fit([stemmed_sentence1, stemmed_sentence2])\n    vectorized_sentences = vectorizer.transform([stemmed_sentence1, stemmed_sentence2])\n\n    # Calculate cosine similarity\n    cosine_sim = cosine_similarity(vectorized_sentences)[0][1]\n\n    return cosine_sim\n\n\ndef extract_keywords_from_text1(text):\n    # Extract keywords from the given text\n    KeyBERT1 = kw_tool.extract_keywords(text, keyphrase_ngram_range=(1,1), top_n=10)\n    KeyBERT2 = kw_tool.extract_keywords(text, keyphrase_ngram_range=(2,2), top_n=10)\n\n    # Combine all extracted keywords\n    all_keywords = [key[0] for key in KeyBERT1] + \\\n                   [key[0] for key in KeyBERT2] \n    # Filter out empty keywords\n    all_keywords = [keyword for keyword in all_keywords if keyword]\n\n    # Filter out very similar keywords\n    similarity_threshold_between_keywords = 0.4  # Threshold for similarity between keywords\n    unique_keywords = []\n    for keyword in all_keywords:\n        if all(calculate_similarity(keyword, existing_keyword) < similarity_threshold_between_keywords for existing_keyword in unique_keywords):\n            unique_keywords.append(keyword)\n\n    return unique_keywords","metadata":{"id":"c7a1388c","outputId":"ddd01415-06d2-41eb-e2f3-8fbafb9ff50d","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_keywords1=extract_keywords_from_text1(N_text)\nunique_keywords1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pke\n\ndef extract_keywords_from_text2(text):\n    # Initialize keyphrase extraction model, here TopicRank\n    extractor = pke.unsupervised.TopicRank()\n\n    # Load the content of the document\n    extractor.load_document(input=text, language='en')\n\n    # Keyphrase candidate selection: in the case of TopicRank: sequences of nouns\n    # and adjectives (i.e., `(Noun|Adj)*`)\n    extractor.candidate_selection()\n\n    # Candidate weighting: using a random walk algorithm\n    extractor.candidate_weighting()\n\n    # N-best selection, keyphrases contains the 10 highest scored candidates\n    keyphrases = extractor.get_n_best(n=20)\n\n    # Extract keyphrases\n    keywords = [keyphrase for keyphrase, score in keyphrases]\n\n    # Calculate similarity with keywords\n    unique_keywords = []\n\n    # Handling unigrams and bigrams separately\n    unigrams = [keyphrase for keyphrase in keywords if len(keyphrase.split()) == 1]\n    bigrams = [keyphrase for keyphrase in keywords if len(keyphrase.split()) == 2]\n\n    # Add unigrams to unique_keywords directly\n    unique_keywords.extend(unigrams)\n\n    # Filter bigrams based on similarity with existing keywords\n    for keyphrase in bigrams:\n        similarity = calculate_similarity(keyphrase, ' '.join(keywords))\n        if similarity < 0.4:  # Adjust the similarity threshold as needed\n            unique_keywords.append(keyphrase)\n\n    return unique_keywords","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_keywords2=extract_keywords_from_text2(N_text)\nunique_keywords2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generate Distractors","metadata":{"id":"05818b09","jp-MarkdownHeadingCollapsed":true,"tags":[]}},{"cell_type":"code","source":"from sense2vec import Sense2Vec\n# load sense2vec vectors\ns2v = Sense2Vec().from_disk('s2v_old')","metadata":{"id":"86fe680f","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import OrderedDict\n\ndef sense2vec_get_words(word, s2v):\n    output = []\n    word = word.lower()\n\n    sense = s2v.get_best_sense(word)\n    similarity_threshold = 0.3\n    out = []\n    \n    if sense is not None:\n        most_similar = s2v.most_similar(sense, n=20)\n        for sim in most_similar:\n            append_word= sim[0].split(\"|\")[0].replace(\"_\", \" \").lower()\n\n            # Check similarity with keyword\n            similarity_keyword = calculate_similarity(word, append_word)\n            #print(f\"Similarity between '{word}' and '{append_word}': {similarity_keyword}\")\n\n            # Check if similarity with keyword is above the threshold\n            if similarity_keyword >= similarity_threshold:\n                continue\n            \n            # Check similarity with existing distractors\n            similarity_to_existing = [calculate_similarity(append_word, existing_distractor) for existing_distractor in output]\n\n            # Check if similarity with any existing distractor is above the threshold\n            if any(similarity >= similarity_threshold for similarity in similarity_to_existing):\n                continue\n            \n            # If the conditions are met, append the word to the list of output\n            output.append(append_word.title())\n\n        out = list(OrderedDict.fromkeys(output))\n    return out[:3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for word in unique_keywords:\n    existing_distractor=sense2vec_get_words(word, s2v)\n    print(word , existing_distractor)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### At Inference Time","metadata":{}},{"cell_type":"code","source":"#general question generation model\nimport torch\nfrom peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nHUGGING_FACE_USER_NAME=''\nmodel_name=''\npeft_model_id = f\"{HUGGING_FACE_USER_NAME}/{model_name}\"\nconfig = PeftConfig.from_pretrained(peft_model_id)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=False, device_map='auto')\nG_tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n\n# Load the Lora model\nG_model = PeftModel.from_pretrained(model, peft_model_id)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_questions(context, answer, distractors):\n    device = next(G_model.parameters()).device\n    input_text = f\"Given the context '{context}' and the answer '{answer}' , what question can be asked?\"\n    encoding = G_tokenizer.encode_plus(input_text, padding=True,truncation=True, return_tensors=\"pt\").to(device)\n\n    output_tokens = G_model.generate(**encoding, early_stopping=True, num_beams=5, num_return_sequences=1, no_repeat_ngram_size=2, max_length=200)\n    question = G_tokenizer.decode(output_tokens[0], skip_special_tokens=True).replace(\"question:\", \"\").strip()\n    return question","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context_printed = False  # Flag to keep track of whether the context has been printed\n\n# Iterate over unique keywords and generate questions for keywords with distractor lists\nfor word in unique_keywords:\n    existing_distractors = sense2vec_get_words(word, s2v)\n    if existing_distractors:\n        if not context_printed:\n            print(\"Context:\", N_text)\n            print()\n            context_printed = True  # Set the flag to True after printing the context\n        \n        context = N_text\n        answer = word\n        distractors = existing_distractors\n        question = generate_questions(context, answer, existing_distractors)\n        \n        print(\"Answer:\", answer)\n        print(\"Distractors:\", existing_distractors)\n        print(\"Generated Question:\", question)\n        print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_and_generate_questions(file_path):\n    # Read the data from the file\n    example = read_data(file_path)\n    N_text_file, keywords, keyword_question_distractors = \"\", [], []\n\n    # Check if example is not None and not empty\n    if example is not None and example.strip():\n        # Remove empty lines between sentences\n        cleaned_text = '\\n'.join(line.strip() for line in example.split('\\n') if line.strip())\n\n        # Concatenate lines and separate them with a period\n        concatenated_text = '.'.join(cleaned_text.split('\\n'))\n\n        # Tokenize the text using G_tokenizer\n        tokens = G_tokenizer.tokenize(concatenated_text)\n\n        # Check if the number of tokens is less than 1024\n        if len(tokens) < 1024: # or 512 for google-flan-t5\n            # Normalize the concatenated text\n            N_text_file = normalize_text(concatenated_text)\n\n            # Check if N_text_file has text\n            if N_text_file:\n                keywords = extract_keywords_from_text(N_text_file)\n                if keywords:\n                    for word in keywords:\n                        current_distractors = sense2vec_get_words(word, s2v)\n                        if current_distractors:\n                            question = generate_questions(N_text_file, word)\n                            keyword_question_distractors.append((word, current_distractors, question))\n                else:\n                    print(\"No keywords generated.\")\n            else:\n                print(\"No text available.\")\n        else:\n            print(\"The tokenized text has more than 1024 tokens.\")\n    else:\n        print(\"The file is empty or large.\")\n        # Handle the case where the file is empty or cannot be loaded\n        \n    result = [(N_text_file, keyword_question_distractors)]\n\n    return result\n\n# Example usage:\nfile_path = \"\"\nTotal_List = process_and_generate_questions(file_path)\n\n# Print the context, keyword, question, and distractors\nfor context, keyword_question_distractors in Total_List:\n    if context:\n        print(f\"context: {context}\")\n        print()\n        for keyword, distractors, question in keyword_question_distractors:\n            print(f\"Keyword: {keyword}\")\n            print(f\"Question: {question}\")\n            print(f\"Distractors: {distractors}\")\n            print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ###########################################","metadata":{}},{"cell_type":"markdown","source":"# For Training Time","metadata":{}},{"cell_type":"markdown","source":"### loading the model and the tokenizer","metadata":{"id":"f30fd042","tags":[]}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig\n\nmodel_name_or_path = \"facebook/bart-base\" # google/flan-t5-base\n\n# Define the quantization configuration with 4-bit\n# quantization_config = BitsAndBytesConfig(bit_width=4, bnb_4bit_compute_type=\"torch.float16\")\n\n# Load the model with the specified quantization configuration\nG_model = AutoModelForSeq2SeqLM.from_pretrained(\n    model_name_or_path,\n    torch_dtype=torch.float32,\n    device_map='auto',\n)\nG_tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4c008ac5-d6ca-4569-a4d7-d6bf53503205","outputId":"c91ef1cf-e4c7-4086-c297-3f02a4316622","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, TaskType, get_peft_model\nLora_config = LoraConfig(\n    r=18,\n    lora_alpha=12,\n    target_modules=[\"q_proj\", \"v_proj\"], # or q and v for other models\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.SEQ_2_SEQ_LM\n)\n\nL_model = get_peft_model(G_model, Lora_config)\nprint(L_model.print_trainable_parameters())","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dba4b9f7","outputId":"8c3ff1fb-3fc8-470d-b489-130ae05da02e","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset, load_dataset\n\n#for diff english data\nen = (\n    load_dataset(\"mou3az/Question-Answering-Generation-Choices\", split=\"train\")\n    .filter(lambda example: len(example[\"context\"]) < 951)\n)\n# # Split the filtered dataset into training and validation sets\nen_train = en.select(range(19500))  # Select first 15,000 examples for training\nen_validation = en.select(range(19500, 20901))  # Select next 2,500 examples for validation","metadata":{"id":"55751b0d","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#For General data\n# import ast\ndef create_prompt1(context, answer):\n    input_text = f\"Given the context '{context}' and the answer '{answer}' , what question can be asked?\"\n    return input_text\n\ndef create_prompt2(question):\n    output_text = f\"question: {question}\"\n    return output_text","metadata":{"id":"85d18ac3","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for english data\nen_train_data = en_train.map(lambda samples: G_tokenizer.encode_plus(create_prompt1(samples['context'], samples['answer']), padding=True), remove_columns=[\"context\", \"answer\", \"question\",\"distractors\"])\nen_validation_data = en_validation.map(lambda samples: G_tokenizer.encode_plus(create_prompt1(samples['context'], samples['answer']), padding=True), remove_columns=[\"context\", \"answer\", \"question\",'distractors'])\nen_question_Tdata = en_train.map(lambda samples: G_tokenizer.encode_plus(create_prompt2(samples['question']), padding=True), remove_columns=[\"context\", \"answer\", \"question\",'distractors'])[\"input_ids\"]\nen_question_Vdata = en_validation.map(lambda samples: G_tokenizer.encode_plus(create_prompt2(samples['question']), padding=True), remove_columns=[\"context\", \"answer\", \"question\",'distractors'])[\"input_ids\"]\nen_train_data=en_train_data.add_column(\"labels\", en_question_Tdata)\nen_validation_data=en_validation_data.add_column(\"labels\", en_question_Vdata)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["6f3ec6892726467ebe3fae7f102fae16","22018e6ad60c48dfa066b054320081e6","b2640b2b37be445e937ca6eea99087e7","bf7d55d6228949a9a2363050553a79ce","63bfa208041a41bb9ad6068bed803a45","4dafa2ee5faf4509912f7a68744d5a90","57a6ed1d171840c1af8383198f0ec4f1","d3b411705e7947d7a534cc67e75a7b5e","b69da5d4e1384585a28d6fefef280137","aaf7271fef0b4032901703c97002c610","8aea77791d3340c6ba4e236739b5a0c3"]},"id":"c60d0004","outputId":"9798760e-5a66-4c4f-f49b-a0041f02dda9","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, EarlyStoppingCallback, Seq2SeqTrainer\n\ntraining_args = Seq2SeqTrainingArguments(\n      gradient_accumulation_steps=10,\n      per_device_train_batch_size=45,\n      per_device_eval_batch_size=45,\n      # save_steps=2,\n      eval_steps=150,\n      warmup_steps=150,\n      logging_steps=150,\n      weight_decay=0.05,\n      # save_total_limit=5,\n      learning_rate=3e-3,\n      max_steps=3000,\n      # num_train_epochs=2,\n      # load_best_model_at_end=True,\n      # gradient_checkpointing=True,\n      lr_scheduler_type=\"linear\",\n      do_train=True,\n      do_eval=True,\n      # fp16=False,\n      report_to=\"all\",\n      log_level=\"debug\",\n      logging_dir='./logs',\n      output_dir='./outputs',\n      label_names=[\"labels\"],\n      evaluation_strategy=\"steps\",\n      # metric_for_best_model=\"eval_loss\",\n    )\n\ntrainer = Seq2SeqTrainer(\n    model=L_model,\n    args=training_args,\n    tokenizer=G_tokenizer,\n    train_dataset=en_train_data,\n    eval_dataset=en_validation_data,\n    # callbacks=[EarlyStoppingCallback(2, 1.0)],\n    data_collator=DataCollatorForSeq2Seq(G_tokenizer,label_pad_token_id=-100),\n)\n\n# Additional configuration\nL_model.config.use_cache = False\ntorch.cuda.empty_cache()\n# L_model.config.bnb_8bit_compute_type = \"torch.float16\"\n\n# Start training\ntrainer.train()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"16c9fd76","outputId":"5d6528e0-decb-48fe-c371-cfcdc6bbf84c","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to hugging face\nmodel_name = \"\"\nHUGGING_FACE_USER_NAME = \"\"\n\nL_model.push_to_hub(f\"{HUGGING_FACE_USER_NAME}/{model_name}\", token='')","metadata":{"id":"1d90c56e","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save model checkpoint\nL_model.save_pretrained(\"\")\n# Create a zip archive\n!zip -r saved_model.zip VF","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#general question generation model\nimport torch\nfrom peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nHUGGING_FACE_USER_NAME=''\nmodel_name=''\npeft_model_id = f\"{HUGGING_FACE_USER_NAME}/{model_name}\"\nconfig = PeftConfig.from_pretrained(peft_model_id)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=False, device_map='auto')\nG_tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n\n# Load the Lora model\nL_model = PeftModel.from_pretrained(model, peft_model_id)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_questions(context, answer):\n    device = next(L_model.parameters()).device\n    input_text = f\"Given the context '{context}' and the answer '{answer}', what question can be asked?\"\n    encoding = G_tokenizer.encode_plus(input_text, padding=True, return_tensors=\"pt\").to(device)\n\n    output_tokens = L_model.generate(\n        **encoding,\n        early_stopping=True,\n        do_sample= True,\n        num_beams=5,\n        num_return_sequences=1,\n        no_repeat_ngram_size=2,\n        max_length=256,\n        temperature=0.6,\n        top_p=0.95,\n        repetition_penalty=1.2\n    )\n    question = G_tokenizer.decode(output_tokens[0], skip_special_tokens=True).replace(\"question :\", \"\").strip()\n    return question","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\nfrom rouge import Rouge\nfrom fuzzywuzzy import fuzz\nfrom bert_score import score\n\n\ndef calculate_bleu_scores(references, predictions):\n    return corpus_bleu([[ref.split()] for ref in references], [pred.split() for pred in predictions])\n\ndef calculate_rouge_scores(references, predictions):\n    rouge = Rouge()\n    rouge_scores = rouge.get_scores(predictions, references, avg=True)\n    return rouge_scores\n\ndef calculate_accuracy(references, predictions):\n    accuracies = [fuzz.token_sort_ratio(ref, pred) / 100.0 for ref, pred in zip(references, predictions)]\n    return sum(accuracies) / len(accuracies)\n\ndef calculate_bert_score(references, predictions):\n    P, R, F1 = score(predictions, references, lang='en', verbose=False)\n    return F1.mean().item()\n\ndef evaluate(dataset):\n    references = [sample['question'] for sample in dataset]\n    predictions = [generate_questions(sample['context'], sample['answer']) for sample in dataset]  # Assuming 'generate_questions' generates model's output\n\n    bleu_score = calculate_bleu_scores(references, predictions)\n    rouge_scores = calculate_rouge_scores(references, predictions)\n    accuracy = calculate_accuracy(references, predictions)\n    bert_score = calculate_bert_score(references, predictions)\n\n    print(\"Overall Accuracy:\", accuracy)\n    print(\"Overall BLEU Score:\", bleu_score)\n    print(\"Overall ROUGE Score:\", rouge_scores)\n    print(\"Overall BERTScore:\", bert_score)\n    \n    return accuracy, bleu_score, rouge_scores, bert_score\n\n# Assuming 'en_validation' is your dataset\naccuracy, bleu_score, rouge_scores, bert_score = evaluate(en_validation)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}